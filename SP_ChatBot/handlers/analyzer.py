import io
import os
import re
import tempfile
from aiogram import Dispatcher, F
from aiogram.fsm.context import FSMContext
from aiogram.types import CallbackQuery, Message, InlineKeyboardMarkup, InlineKeyboardButton, \
    BufferedInputFile
from docx import Document as DocxDocument
from sqlalchemy import select
from db.session import SessionLocal
from handlers.checking_criteria import contains_label0_features, contains_label1_features
from keyboards.inline import get_analysis_menu_keyboard, get_back_to_analysis_menu_keyboard
from models.bases import InterviewAnalysisResult
from states.fsm import InterviewAnalysis
from transformers import pipeline

analyzer_pipeline = pipeline("text-classification", model="ekas03/my_genspec_analyzer")

def analyzer(dp: Dispatcher):
    @dp.callback_query(F.data == "analysis")
    async def show_analysis_menu(callback: CallbackQuery):
        await callback.message.edit_text(
            "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:",
            reply_markup=get_analysis_menu_keyboard()
        )
        await callback.answer()

    @dp.callback_query(F.data == "my_interviews")
    async def show_user_interviews(callback: CallbackQuery):
        user_id = callback.from_user.id
        async with SessionLocal() as session:
            result = await session.execute(
                select(InterviewAnalysisResult).where(InterviewAnalysisResult.telegram_id == user_id)
            )
            interviews = result.scalars().all()

        if not interviews:
            await callback.message.edit_text("–£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤—å—é!", reply_markup=get_back_to_analysis_menu_keyboard())
            await callback.answer()
            return

        keyboard = InlineKeyboardMarkup(
            inline_keyboard=[[InlineKeyboardButton(text=i.title, callback_data=f"interview_{i.id}")]
                             for i in interviews] + [[InlineKeyboardButton(text="üîô –ù–∞–∑–∞–¥", callback_data="analysis")]]
        )

        await callback.message.edit_text("–í–∞—à–∏ –∏–Ω—Ç–µ—Ä–≤—å—é:", reply_markup=keyboard)
        await callback.answer()

    @dp.callback_query(F.data.startswith("interview_"))
    async def show_interview_details(callback: CallbackQuery):
        interview_id = int(callback.data.split("_")[1])

        async with SessionLocal() as session:
            interview = await session.get(InterviewAnalysisResult, interview_id)

        if not interview:
            await callback.message.answer("–ò–Ω—Ç–µ—Ä–≤—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            return

        msg = (
            f"üìä <b>{interview.title}</b>\n"
            f"–í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {interview.total_sentences}\n"
            f"–û–ë–©–ò–•: {interview.general_count}\n"
            f"–ß–ê–°–¢–ù–´–•: {interview.specific_count}\n\n"
            f"–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ {interview.majority}"
        )

        keyboard = InlineKeyboardMarkup(
            inline_keyboard=[
                [InlineKeyboardButton(text="üì§ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç", callback_data=f"download_{interview_id}")],
                [InlineKeyboardButton(text="üîô –ù–∞–∑–∞–¥", callback_data="my_interviews")]
            ]
        )

        await callback.message.edit_text(msg, reply_markup=keyboard, parse_mode="HTML")
        await callback.answer()

        @dp.callback_query(F.data.startswith("download_"))
        async def send_interview_docx(callback: CallbackQuery):
            interview_id = int(callback.data.split("_")[1])

            async with SessionLocal() as session:
                interview = await session.get(InterviewAnalysisResult, interview_id)

            if not interview:
                await callback.message.answer("–ò–Ω—Ç–µ—Ä–≤—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
                return

            per_general = (interview.general_count / interview.total_sentences) * 100 if interview.total_sentences > 0 else 0
            per_specific = (interview.specific_count / interview.total_sentences) * 100 if interview.total_sentences > 0 else 0

            doc = DocxDocument()
            doc.add_heading(interview.title, 0)
            doc.add_paragraph(f"–í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {interview.total_sentences}")
            doc.add_paragraph(f"–û–ë–©–ò–•: {interview.general_count}, —Ç–æ –µ—Å—Ç—å {per_general:.1f}%\n\n")
            doc.add_paragraph(f"–ß–ê–°–¢–ù–´–•: {interview.specific_count}, —Ç–æ –µ—Å—Ç—å {per_specific:.1f}%\n\n")
            doc.add_paragraph(f"–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ {interview.majority}")
            doc.add_heading("–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–Ω—Ç–µ—Ä–≤—å—é:", level=1)
            doc.add_paragraph(interview.full_text)
            buf = io.BytesIO()
            doc.save(buf)
            buf.seek(0)

            file = BufferedInputFile(buf.read(), filename=f"{interview.title}.docx")
            await callback.message.answer_document(file)
            await callback.answer()

    @dp.callback_query(F.data == "analyze_interview")
    async def start_analysis(callback: CallbackQuery, state: FSMContext):
        await callback.message.edit_text(
            "–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤—å—é:",
            reply_markup=InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="üîô –ù–∞–∑–∞–¥", callback_data="analysis")]]))
        await state.set_state(InterviewAnalysis.waiting_for_title)
        await callback.answer()

    @dp.message(InterviewAnalysis.waiting_for_title)
    async def get_interview_title(message: Message, state: FSMContext):
        await state.update_data(title=message.text)
        await message.answer(
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .docx, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å \"–í:\", –∞ –æ—Ç–≤–µ—Ç—ã —Å \"–û:\"",
            reply_markup=InlineKeyboardMarkup(inline_keyboard=[
        [InlineKeyboardButton(text="üîô –ù–∞–∑–∞–¥", callback_data="analyze_interview")]])
        )
        await state.set_state(InterviewAnalysis.waiting_for_docx)

    @dp.message(InterviewAnalysis.waiting_for_docx, F.document)
    async def get_docx_file(message: Message, state: FSMContext):
        doc = message.document
        if not doc.file_name.endswith(".docx"):
            await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .docx, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å \"–í:\", –∞ –æ—Ç–≤–µ—Ç—ã —Å \"–û:\"")
            return

        with tempfile.TemporaryDirectory() as tmpdir:
            file_path = os.path.join(tmpdir, doc.file_name)
            await message.bot.download(doc, destination=file_path)

            docx = DocxDocument(file_path)
            full_text = "\n".join([para.text for para in docx.paragraphs])

        pattern = re.compile(r"–û:\s*(.*?)(?=\s*–í:|\s*$)", re.DOTALL)
        matches = pattern.findall(full_text)
        if not matches:
            await message.answer("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –º–µ–∂–¥—É '–û:' –∏ '–í:'.")
            return

        interview_text = "\n".join(matches)

        data = await state.get_data()
        title = data.get("title")

        try:
            msg, _ = await analyze_and_save(interview_text, message.from_user.id, title)
        except Exception as e:
            await message.answer(str(e))
            await state.clear()
            return

        await message.answer(msg, reply_markup=get_back_to_analysis_menu_keyboard())
        await state.clear()

def split_into_sentences(text: str) -> list[str]:
    sentences = re.split(r'(?<=[.!?])[\s\n]+', text.strip())
    return [s.strip() for s in sentences if s.strip()]

async def analyze_and_save(text: str, user_id: int, title: str) -> tuple[str, InterviewAnalysisResult]:
        sentences = split_into_sentences(text)
        if not sentences:
            raise ValueError("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.")

        try:
            results = analyzer_pipeline(sentences, truncation=True)
        except Exception as e:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –º–æ–¥–µ–ª–∏: {e}")

        grammar_label0_count = sum(1 for s in sentences if contains_label0_features(s))
        grammar_label1_count = sum(1 for s in sentences if contains_label1_features(s))

        general_count = sum(1 for r in results if r["label"].lower() == "label_0") + grammar_label0_count
        specific_count = sum(1 for r in results if r["label"].lower() == "label_1") + grammar_label1_count
        total = len(results) + grammar_label0_count + grammar_label1_count

        majority = (
            "–±–æ–ª—å—à–µ –û–ë–©–ò–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —á–µ–ª–æ–≤–µ–∫ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –≤–∏–¥–∏—Ç –≤—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –≤ —Ü–µ–ª–æ–º, —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å—É—Ç–∏, –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏!" if general_count > specific_count else
            "–±–æ–ª—å—à–µ –ß–ê–°–¢–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —á–µ–ª–æ–≤–µ–∫ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –∑–∞–º–µ—á–∞–µ—Ç –¥–µ—Ç–∞–ª–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É –∏ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏!" if specific_count > general_count else
            "—Ä–∞–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –û–ë–©–ò–• –∏ –ß–ê–°–¢–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠—Ç–æ –º–æ–∂–µ—Ç –≥–æ–≤–æ—Ä–∏—Ç—å –∏ –æ —Ç–æ–º, —á—Ç–æ –≤ —á–µ–ª–æ–≤–µ–∫–µ –µ—Å—Ç—å –±–∞–ª–∞–Ω—Å \"–æ–±—â–∏—Ö\" –∏ \"—á–∞—Å—Ç–Ω—ã—Ö\" –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–æ –∏ –æ —Ç–æ–º, —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –ø–æ—ç—Ç–æ–º—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∏–Ω—Ç–µ—Ä–≤—å—é –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –µ–≥–æ –Ω–∞ —Ü–∏—Ñ—Ä–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑!"
        )

        per_general = (general_count / total) * 100 if total > 0 else 0
        per_specific = (specific_count / total) * 100 if total > 0 else 0

        result_text = (
            f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ç–µ—Ä–≤—å—é:\n\n"
            f"–í—Å–µ–≥–æ –≤—ã—è–≤–ª–µ–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {total}\n\n"
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –æ—Ç–º–µ—á–µ–Ω–Ω—ã—Ö –∫–∞–∫ GENERAL (–û–ë–©–ï–ï): {general_count}, —Ç–æ –µ—Å—Ç—å {per_general:.1f}%\n\n"
            f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –æ—Ç–º–µ—á–µ–Ω–Ω—ã—Ö –∫–∞–∫ SPECIFIC (–ß–ê–°–¢–ù–û–ï): {specific_count}, —Ç–æ –µ—Å—Ç—å {per_specific:.1f}%)\n\n"
            f"–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∞–Ω–∞–ª–∏–∑–∞ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ {majority}"
        )

        result = InterviewAnalysisResult(
            title=title,
            telegram_id=user_id,
            total_sentences=total,
            general_count=general_count,
            specific_count=specific_count,
            majority=majority,
            full_text=text
        )

        async with SessionLocal() as session:
            session.add(result)
            await session.commit()

        return result_text, result